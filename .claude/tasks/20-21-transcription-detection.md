# Task 20-21: Transcription Whisper & D√©tection Contexte

> **Dur√©e estim√©e** : 3.5h  
> **Phase** : Core - Consultation  
> **Feature PRD** : √âpic 1 - US-001, US-002 (Transcription & D√©tection contexte)

## Contexte

L'audio enregistr√© est envoy√© √† l'API Whisper d'OpenAI pour transcription. Ensuite, le texte est analys√© pour d√©tecter automatiquement s'il s'agit d'une consultation ou d'une intervention chirurgicale.

## Objectif

Impl√©menter l'appel √† Whisper API et la d√©tection automatique du contexte m√©dical.

## Scope

### Inclus ‚úÖ
- API Route pour transcription Whisper
- Affichage streaming de la transcription
- √âditeur de transcription
- D√©tection contexte (consultation vs intervention)
- Indicateur visuel du contexte d√©tect√©
- Correction manuelle du contexte

### Exclus ‚ùå
- G√©n√©ration du CR (Task 22)
- Stockage long terme de l'audio

---

## Impl√©mentation

### √âtape 1 : API Route Whisper

**app/api/transcribe/route.ts**
```typescript
import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export async function POST(request: NextRequest) {
  try {
    const formData = await request.formData();
    const audioFile = formData.get('audio') as File;

    if (!audioFile) {
      return NextResponse.json(
        { error: 'No audio file provided' },
        { status: 400 }
      );
    }

    // Convertir le File en format compatible avec OpenAI
    const transcription = await openai.audio.transcriptions.create({
      file: audioFile,
      model: 'whisper-1',
      language: 'fr',
      prompt: 'Transcription m√©dicale ORL. Vocabulaire: otoscopie, rhinoscopie, pharynx, larynx, tympan, amygdales, v√©g√©tations, septoplastie, FESS, paracent√®se.',
    });

    return NextResponse.json({
      text: transcription.text,
    });
  } catch (error) {
    console.error('Transcription error:', error);
    return NextResponse.json(
      { error: 'Transcription failed' },
      { status: 500 }
    );
  }
}

export const config = {
  api: {
    bodyParser: false,
  },
};
```

### √âtape 2 : API Route D√©tection Contexte

**app/api/detect-context/route.ts**
```typescript
import { NextRequest, NextResponse } from 'next/server';
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

const CONTEXT_DETECTION_PROMPT = `Tu es un assistant m√©dical sp√©cialis√© ORL. Analyse le texte suivant et d√©termine s'il s'agit d'une CONSULTATION ou d'une INTERVENTION chirurgicale.

Crit√®res pour INTERVENTION:
- Mention d'anesth√©sie (g√©n√©rale, locale, s√©dation)
- Vocabulaire chirurgical (incision, dissection, suture, h√©mostase)
- Termes de bloc op√©ratoire (installation, drapage, bistouri)
- Types d'interventions: amygdalectomie, septoplastie, FESS, paracent√®se, pose d'a√©rateurs

Crit√®res pour CONSULTATION:
- Examen clinique (otoscopie, rhinoscopie, palpation)
- Sympt√¥mes du patient (douleur, vertiges, acouph√®nes, surdit√©)
- Diagnostic et prescription
- Suivi post-op√©ratoire simple

R√©ponds UNIQUEMENT avec un JSON valide:
{
  "context": "consultation" | "intervention",
  "confidence": 0.0-1.0,
  "keywords_detected": ["mot1", "mot2", ...]
}`;

export async function POST(request: NextRequest) {
  try {
    const { transcription } = await request.json();

    if (!transcription) {
      return NextResponse.json(
        { error: 'No transcription provided' },
        { status: 400 }
      );
    }

    const message = await anthropic.messages.create({
      model: 'claude-3-haiku-20240307', // Rapide et √©conomique pour cette t√¢che
      max_tokens: 256,
      messages: [
        {
          role: 'user',
          content: `${CONTEXT_DETECTION_PROMPT}\n\nTexte √† analyser:\n${transcription}`,
        },
      ],
    });

    const responseText = message.content[0].type === 'text' 
      ? message.content[0].text 
      : '';

    // Parser le JSON
    const jsonMatch = responseText.match(/\{[\s\S]*\}/);
    if (!jsonMatch) {
      return NextResponse.json({
        context: 'unknown',
        confidence: 0,
        keywords_detected: [],
      });
    }

    const result = JSON.parse(jsonMatch[0]);

    return NextResponse.json(result);
  } catch (error) {
    console.error('Context detection error:', error);
    return NextResponse.json(
      { context: 'unknown', confidence: 0, keywords_detected: [] },
      { status: 200 } // Retourner unknown plut√¥t qu'une erreur
    );
  }
}
```

### √âtape 3 : Hook useTranscription

**src/features/consultation/hooks/use-transcription.ts**
```typescript
import { useMutation } from '@tanstack/react-query';
import { useConsultationStore } from '@/stores/consultation-store';

interface TranscriptionResult {
  text: string;
}

interface ContextResult {
  context: 'consultation' | 'intervention' | 'unknown';
  confidence: number;
  keywords_detected: string[];
}

export function useTranscription() {
  const { 
    setTranscription, 
    setContextDetected, 
    setIsTranscribing,
    draft 
  } = useConsultationStore();

  const transcribeMutation = useMutation({
    mutationFn: async (audioBlob: Blob) => {
      setIsTranscribing(true);
      
      const formData = new FormData();
      formData.append('audio', audioBlob, 'recording.webm');

      const response = await fetch('/api/transcribe', {
        method: 'POST',
        body: formData,
      });

      if (!response.ok) {
        throw new Error('Transcription failed');
      }

      return response.json() as Promise<TranscriptionResult>;
    },
    onSuccess: async (data) => {
      setTranscription(data.text);
      
      // D√©tecter le contexte automatiquement
      try {
        const contextResponse = await fetch('/api/detect-context', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ transcription: data.text }),
        });
        
        const contextResult: ContextResult = await contextResponse.json();
        setContextDetected(contextResult.context);
      } catch {
        setContextDetected('unknown');
      }
      
      setIsTranscribing(false);
    },
    onError: () => {
      setIsTranscribing(false);
    },
  });

  return {
    transcribe: transcribeMutation.mutate,
    isTranscribing: transcribeMutation.isPending,
    error: transcribeMutation.error,
  };
}
```

### √âtape 4 : Composant Transcription Editor

**src/features/consultation/components/transcription-step.tsx**
```typescript
'use client';

import { useEffect, useState } from 'react';
import { Button } from '@/components/ui/button';
import { Textarea } from '@/components/ui/textarea';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { RadioGroup, RadioGroupItem } from '@/components/ui/radio-group';
import { Label } from '@/components/ui/label';
import { Loader2, ArrowRight, ArrowLeft, Stethoscope, Scissors, HelpCircle } from 'lucide-react';
import { useConsultationStore } from '@/stores/consultation-store';
import { useTranscription } from '../hooks/use-transcription';
import { cn } from '@/lib/utils';

export function TranscriptionStep() {
  const {
    draft,
    isTranscribing,
    setTranscription,
    setContextDetected,
    completeStep,
    setStep,
  } = useConsultationStore();

  const { transcribe } = useTranscription();
  const [localTranscription, setLocalTranscription] = useState(draft.transcription);

  // Lancer la transcription automatiquement si on a un audio
  useEffect(() => {
    if (draft.audioBlob && !draft.transcription && !isTranscribing) {
      transcribe(draft.audioBlob);
    }
  }, [draft.audioBlob, draft.transcription, isTranscribing, transcribe]);

  // Sync avec le store
  useEffect(() => {
    setLocalTranscription(draft.transcription);
  }, [draft.transcription]);

  const handleContinue = () => {
    setTranscription(localTranscription);
    completeStep('transcription');
    setStep('generation');
  };

  const contextOptions = [
    { value: 'consultation', label: 'Consultation', icon: Stethoscope, color: 'text-blue-600' },
    { value: 'intervention', label: 'Intervention', icon: Scissors, color: 'text-orange-600' },
    { value: 'unknown', label: 'Non d√©termin√©', icon: HelpCircle, color: 'text-gray-600' },
  ];

  return (
    <div className="space-y-6">
      {/* Transcription en cours */}
      {isTranscribing && (
        <Card>
          <CardContent className="py-12">
            <div className="flex flex-col items-center gap-4">
              <Loader2 className="h-12 w-12 animate-spin text-primary" />
              <div className="text-center">
                <p className="font-medium">Transcription en cours...</p>
                <p className="text-sm text-muted-foreground">
                  Analyse de l'enregistrement audio
                </p>
              </div>
            </div>
          </CardContent>
        </Card>
      )}

      {/* Transcription termin√©e */}
      {!isTranscribing && (
        <>
          {/* Contexte d√©tect√© */}
          <Card>
            <CardHeader>
              <CardTitle className="text-lg flex items-center gap-2">
                Type de document
                {draft.contextDetected !== 'unknown' && (
                  <Badge variant="outline" className="ml-2">
                    D√©tect√© automatiquement
                  </Badge>
                )}
              </CardTitle>
            </CardHeader>
            <CardContent>
              <RadioGroup
                value={draft.contextDetected}
                onValueChange={(v) => setContextDetected(v as any)}
                className="grid grid-cols-3 gap-4"
              >
                {contextOptions.map((option) => (
                  <Label
                    key={option.value}
                    htmlFor={option.value}
                    className={cn(
                      'flex items-center gap-3 p-4 border rounded-lg cursor-pointer transition-colors',
                      draft.contextDetected === option.value
                        ? 'border-primary bg-primary/5'
                        : 'border-muted hover:bg-muted/50'
                    )}
                  >
                    <RadioGroupItem value={option.value} id={option.value} />
                    <option.icon className={cn('h-5 w-5', option.color)} />
                    <span>{option.label}</span>
                  </Label>
                ))}
              </RadioGroup>
            </CardContent>
          </Card>

          {/* √âditeur de transcription */}
          <Card>
            <CardHeader>
              <CardTitle className="text-lg">Transcription</CardTitle>
            </CardHeader>
            <CardContent>
              <Textarea
                value={localTranscription}
                onChange={(e) => setLocalTranscription(e.target.value)}
                placeholder="La transcription appara√Ætra ici, ou saisissez manuellement..."
                className="min-h-[200px] font-mono text-sm"
              />
              <p className="text-sm text-muted-foreground mt-2">
                Vous pouvez modifier la transcription avant de continuer
              </p>
            </CardContent>
          </Card>
        </>
      )}

      {/* Navigation */}
      <div className="flex justify-between">
        <Button variant="outline" onClick={() => setStep('recording')}>
          <ArrowLeft className="mr-2 h-4 w-4" />
          Retour
        </Button>
        
        <Button 
          onClick={handleContinue} 
          disabled={!localTranscription.trim() || isTranscribing}
        >
          G√©n√©rer le CR
          <ArrowRight className="ml-2 h-4 w-4" />
        </Button>
      </div>
    </div>
  );
}
```

### √âtape 5 : Installation OpenAI SDK

```bash
pnpm add openai @anthropic-ai/sdk
```

### √âtape 6 : Variables d'environnement

Ajouter dans `.env.local` :
```env
OPENAI_API_KEY="sk-..."
ANTHROPIC_API_KEY="sk-ant-..."
```

---

## Crit√®res de succ√®s

### Fonctionnels
- [ ] Transcription Whisper retourne le texte
- [ ] Vocabulaire m√©dical ORL reconnu
- [ ] Contexte d√©tect√© automatiquement
- [ ] √âdition de la transcription possible
- [ ] Correction manuelle du contexte

### Techniques
- [ ] Latence transcription < 10s pour 3min audio
- [ ] Gestion des erreurs API
- [ ] D√©tection contexte fiable (> 90%)

### M√©triques PRD
- [ ] Pr√©cision transcription > 95%
- [ ] D√©tection contexte > 98%

---

## D√©pendances

### Requiert

| Task | Raison |
|------|--------|
| 18-19 | Audio enregistr√© disponible |

### Bloque

| Task | Raison |
|------|--------|
| 22 | G√©n√©ration CR utilise la transcription |

---

## Notes d'impl√©mentation

### ‚ö†Ô∏è Points d'attention

- **API Routes Next.js** : Transcription et d√©tection sont g√©r√©es c√¥t√© frontend (pas via Xano)
- **Cl√©s API** : `OPENAI_API_KEY` et `ANTHROPIC_API_KEY` dans `.env.local`
- **Format audio** : Whisper accepte `webm/opus`, pas besoin de conversion
- **Sauvegarde Xano** : La transcription n'est pas encore sauvegard√©e √† cette √©tape (fait en Task 22-23)

### üí° Suggestions

- Utiliser `claude-3-haiku` pour la d√©tection contexte (rapide et √©conomique)
- Ajouter le prompt m√©dical ORL √† Whisper pour am√©liorer la reconnaissance
- Impl√©menter un retry en cas d'√©chec de l'API OpenAI

---

## üì° Architecture API

### API Routes Next.js (ce task)

| Route | M√©thode | Description | API externe |
|-------|---------|-------------|-------------|
| `/api/transcribe` | POST | Transcription audio ‚Üí texte | OpenAI Whisper |
| `/api/detect-context` | POST | D√©tection consultation/intervention | Anthropic Claude |

### Endpoints Xano (utilis√©s plus tard)

| Endpoint | M√©thode | Description | Auth |
|----------|---------|-------------|------|
| `/consultation-update/{consultation_id}/transcription` | PATCH | Sauvegarder la transcription | ‚úÖ Bearer |

### Variables d'environnement requises

```env
OPENAI_API_KEY="sk-..."
ANTHROPIC_API_KEY="sk-ant-..."
```
